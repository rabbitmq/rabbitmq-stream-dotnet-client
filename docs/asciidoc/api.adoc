:test-examples: ../Documentation
[[rabbitmq-stream-net-api]]
=== RabbitMQ Stream .NET API

==== Overview

This section describes the API to connect to the RabbitMQ Stream Plugin, publish messages, and consume messages.
There are 3 main interfaces:

* `RabbitMQ.Stream.Client` for connecting to a node and optionally managing streams.
* `RabbitMQ.Stream.Client.Reliable.Producer` to publish messages.
* `RabbitMQ.Stream.Client.Reliable.Consumer` to consume messages.

==== StreamSystem

===== Creating the StreamSystem

The environment is the main entry point to a node or a cluster of nodes. `Producer` and
`Consumer` instances need an `StreamSystem` instance.
Here is the simplest way to create an `StreamSystem` instance:

.Creating an environment with all the defaults
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=create-simple]
--------

<1> Create an environment that will connect to localhost:5552
<2> Close the environment after usage

Note the streamSystem must be closed to release resources when it is no longer needed.

Consider the environment like a long-lived object.
An application will usually create one `StreamSystem` instance when it starts up and close it when it exits.

It is possible to use a multiple end-points to connect to a cluster of nodes.
The:

.Creating an streamSystem with multiple end-points
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=create-multi-endpoints]
--------

<1>  Define the end-points to connect to

By specifying several endpoints, the system will try to connect to the first one, and will pick a new endpoint randomly in case of disconnection.

[[understanding-connection-logic]]
===== Understanding Connection Logic

Creating the StreamSystem to connect to a cluster node works usually seamlessly.
Creating publishers and consumers can cause problems as the client uses hints from the cluster to find the nodes where stream leaders and replicas are located to connect to the appropriate nodes.

These connection hints can be accurate or less appropriate depending on the infrastructure.
If you hit some connection problems at some point – like hostnames impossible to resolve for client applications - this https://blog.rabbitmq.com/posts/2021/07/connecting-to-streams/[blog post] should help you understand what is going on and fix the issues.

===== Enabling TLS

The default TLS port is 5551.

.Creating an StreamSystem that uses TLS
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=create-tls]
--------

<1> Enable TLS
<2> Load certificate authority (CA) certificate from PEM file

.Creating a TLS environment that trusts all server certificates for development
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=create-tls-trust]
--------

<1> Trust all server certificates

===== Configuring the Stream System

The following table sums up the main settings to create an `StreamSystem` using the `StreamSystemConfig`:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Default

|`UserName`
|User name to use to connect.
|`guest`

|`Password`
|Password to use to connect.
|`guest`

|`VirtualHost`
|Virtual host to connect to.
|`/`

|`ClientProvidedName`
| To identify the client in the management UI.
|`dotnet-stream-locator`

|`Heartbeat`
|Time between heartbeats.
|`1 minute`

|`Endpoints`
|The list of endpoints to connect to.
|`localhost:5552`


|`addressResolver`
|Contract to change resolved node address to connect to.
|Pass-through (no-op)

|`Ssl`
|Configuration helper for TLS.
|`null`

|===

===== When a Load Balancer is in Use

A load balancer can misguide the client when it tries to connect to nodes that host stream leaders and replicas.
The https://blog.rabbitmq.com/posts/2021/07/connecting-to-streams/["Connecting to Streams"] blog post covers why client applications must connect to the appropriate nodes in a cluster and how a https://blog.rabbitmq.com/posts/2021/07/connecting-to-streams/#with-a-load-balancer[load balancer can make things complicated] for them.

The `StreamSystemConfig#AddressResolver(AddressResolver)` method allows intercepting the node resolution after metadata hints and before connection.
Applications can use this hook to ignore metadata hints and always use the load balancer, as illustrated in the following snippet:

.Using a custom address resolver to always use a load balancer
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=create-address-resolver]
--------

<1> Set the load balancer address
<2> Use load balancer address for initial connection
<3> Set the endpoints based on `AddressResolver`

The blog post covers the https://blog.rabbitmq.com/posts/2021/07/connecting-to-streams/#client-workaround-with-a-load-balancer[underlying details of this workaround].

===== Managing Streams

Streams are usually long-lived, centrally-managed entities, that is, applications are not supposed to create and delete them.
It is nevertheless possible to create and delete stream with the `StreamSystem`.
This comes in handy for development and testing purposes.

Streams are created with the `StreamSystem.CreateStream(..)` method:

.Creating a stream
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=stream-creation]
--------

<1> Create the `my-stream` stream

`StreamSystem.Create` is idempotent: trying to re-create a stream with the same name and same properties (e.g. maximum size, see below) will not throw an exception.
In other words, you can be sure the stream has been created once `StreamSystem.Create` returns.
Note it is not possible to create a stream with the same name as an existing stream but with different properties.
Such a request will result in an exception.

Streams can be deleted with the `StreamSystem#Delete(String)` method:

.Deleting a stream
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=stream-deletion]
--------

<1> Delete the `my-stream` stream

Note you should avoid stream churn (creating and deleting streams repetitively) as their creation and deletion imply some significant housekeeping on the server side (interactions with the file system, communication between nodes of the cluster).

[[limiting-the-size-of-a-stream]]
It is also possible to limit the size of a stream when creating it.
A stream is an append-only data structure and reading from it does not remove data.
This means a stream can grow indefinitely.
RabbitMQ Stream supports a size-based and time-based retention policies: once the stream reaches a given size or a given age, it is truncated (starting from the beginning).

[IMPORTANT]
.Limit the size of streams if appropriate!
====
Make sure to set up a retention policy on potentially large streams if you don't want to saturate the storage devices of your servers.
Keep in mind that this means some data will be erased!
====

It is possible to set up the retention policy when creating the stream:

.Setting the retention policy when creating a stream
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=stream-creation-retention]
--------

<1> Set the maximum size to 10 GB
<2> Set the segment size to 500 MB

The previous snippet mentions a segment size.
RabbitMQ Stream does not store a stream in a big, single file, it uses segment files for technical reasons.
A stream is truncated by deleting whole segment files (and not part of them)so the maximum size of a stream is usually significantly higher than the size of segment files. 500 MB is a reasonable segment file size to begin with.

[NOTE]
.When does the broker enforce the retention policy?
====
The broker enforces the retention policy when the segments of a stream roll over, that is when the current segment has reached its maximum size and is closed in favor of a new one.
This means the maximum segment size is a critical setting in the retention mechanism.
====

RabbitMQ Stream also supports a time-based retention policy: segments get truncated when they reach a certain age.
The following snippet illustrates how to set the time-based retention policy:

.Setting a time-based retention policy when creating a stream
[source,c#,indent=0]
--------
include::{test-examples}/StreamSystemUsage.cs[tag=stream-creation-time-based-retention]
--------

<1> Set the maximum age to 6 hours
<2> Set the segment size to 500 MB

[[creating-a-producer]]
==== Producer

===== Creating a Producer

A `Producer` instance is created with `Producer.Create`.
The only mandatory setting to specify is the stream to publish to:

.Creating a producer from the environment
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=producer-creation]
--------

<1> Use `Producer.Create` to define the producer
<2> Specify the stream to publish to
<3> Close the producer after usage

Consider a `Producer` instance like a long-lived object, do not create one to send just one message.

[NOTE]
.Producer thread safety
====
`Producer` instances are thread-safe when `Reference` is not set.
Starting from version 1.2.0 the `Reference` field is deprecated.
`Reference` is needed for deduplication see the Deduplication section for more details.
====

Internally, the `StreamSystem` will query the broker to find out about the topology of the stream and will create or re-use a connection to publish to the leader node of the stream.

The following table sums up the main settings to create a `ProducerConfig`:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Default

|`StreamSystem`
|The StreamSystem to use to create the producer.
|No default, mandatory setting.

|`stream`
|The stream to publish to.
|No default, mandatory setting.

|`Reference`
|The logical name of the producer. Specify a name to enable
<<outbound-message-deduplication,message deduplication>>.
|`null` (no deduplication) - Deprecated in version 1.2.0

|`ConfirmationHandler`
|The confirmation handler where received the messages confirmations.
|`null` (no confirmation handler)

|ClientProvidedName
|The TCP connection name to identify the client.
|`dotnet-stream-producer`

|`MaxInFlight`
|The maximum number of messages that can be in flight at any given time. Messages sent - Messages confirmed. To avoid to flood the broker with messages.
|1000

|`ReconnectStrategy`
|The strategy to use when the connection to the broker is lost.
|`BackOffReconnectStrategy`

|`MessagesBufferSize`
|Number of the messages sent for each frame-send.
This value is valid only for the `Send(Message)` method.
|100

|`TimeoutMessageAfter`
|Time to wait before considering a message as not confirmed.
| 3 seconds
// TODO: Document Super stream
|`SuperStreamConfig`
|The super stream configuration.
|`null` (no super stream)

|===

===== Sending Messages

Once a `Producer` has been created, it is possible to send a message with:

* `Producer#send(Message)`, 
* `Producer#send(List<Message>)`
* `Producer#send(List<Message> messages, CompressionType compressionType)`.

The following snippet shows how to publish a message with a byte array payload:

.Sending a message
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=producer-publish]
--------

<1> The payload of a message is an array of bytes.
Messages are not only made of a `byte[]` payload, we will see in
<<working-with-complex-messages,the next section>>
they can also carry pre-defined and application properties.
<2> Send the message.
The method is asynchronous, internally the messages are buffered and sent in batch.
Most of the time you can use this method.
<3> Batch send is synchronous, there is not additional buffering.
The messages are sent immediately.
This method is useful when you want to control the number of the messages to sent is a single frame.
Can be useful in case you need low latency.
<4> Sub entry batching see <<sub-entry-batching-and-compression,Sub Entry Batching>> for more details.
<5> The `ConfirmationHandler` defines an asynchronous callback invoked when the client received from the broker the confirmation the message has been taken into account.
The `ConfirmationHandler` is the place for any logic on publishing confirmation, including re-publishing the message if it is negatively acknowledged.

`MessagesConfirmation` contains the following information:

[%header,cols=2*]
|===

|Parameter Name
|Description

|`Stream`
|The stream the message was published to.

|`PublishingId`
|The publishing id of the message.

|`Status`
|The confirmation status of the message. See <<confirmation-status,Confirmation Status>> for more details.

|`Messages`
|The list of messages that have been confirmed or not.

|===

[[confirmation-status]]
`confirmation.Status` values:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Source

|`ConfirmationStatus.Confirmed`
|The message has been confirmed by the broker.
|Server

|`ConfirmationStatus.Timeout`
| Client gave up waiting for the message
| Client

|StreamNotAvailable
|The stream is not available.
|Server

|InternalError
|The broker encountered an internal error.
|Server

|AccessRefused
|Provided credentials are invalid or you lack permissions for specific vhost/etc.
|Server

|PreconditionFailed
|Catch-all for validation on server (eg. requested to create stream with different parameters but same name).
|Server

|PublisherDoesNotExist
|The publisher does not exist.
|Server

|UndefinedError
|Catch-all for any new status that is not yet handled in the library.
|Server

|===

[WARNING]
.Keep the confirmation callback as short as possible
====
The confirmation callback should be kept as short as possible to avoid blocking the connection thread.
Not doing so can make the `StreamSystem`, `Producer`, `Consumer` instances sluggish or even block them.
Any long processing should be done in a separate thread (e.g. with an asynchronous `Task.Run(....)`).
====

[NOTE]
.Mixing different send methods
====
You can mix different send methods.
For example you can send a message with `send(Message)` and then send a batch of messages with `send(List<Message>)`.
Avoid to sent the `Refence` property in the `ProducerConfig` it enables the deduplication and you could have unexpected results.

`Reference` is deprecated in the version `1.2.0`  see deduplication section for more details.
====

[[working-with-complex-messages]]
===== Working with Complex Messages

The publishing example above showed that messages are made of a byte array payload, but it did not go much further.
Messages in RabbitMQ Stream can actually be more sophisticated, as they comply to the
https://www.amqp.org/resources/specifications[AMQP 1.0 message format].

In a nutshell, a message in RabbitMQ Stream has the following structure:

* properties: _a defined set of standard properties of the message_ (e.g. message ID, correlation ID, content type, etc).
* application properties: a set of arbitrary key/value pairs.
* body: typically an array of bytes.
* message annotations: a set of key/value pairs (aimed at the infrastructure).

The RabbitMQ Stream NET client uses the `Message` class to represent a message.

.Creating a message with properties
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=producer-publish-complex-message]
--------

<1> Get the message
<2> Set the Application properties
<3> Set the message Properties.
You usually don't need to set the properties.

The `Message` contains also the following read-only properties:

* `MessageHeader`
* `Annotations`
* `AmqpValue`

These values are only for compatibility with the AMQP 1.0 message format.

[NOTE]
.Is RabbitMQ Stream based on AMQP 1.0?
====
AMQP 1.0 is a standard that defines _an efficient binary peer-to-peer protocol for transporting messages between two processes over a network_.
It also defines _an abstract message format, with concrete standard encoding_.
This is only the latter part that RabbitMQ Stream uses.
The AMQP 1.0 protocol is not used, only AMQP 1.0 encoded messages are wrapped into the RabbitMQ Stream binary protocol.

The actual AMQP 1.0 message encoding and decoding happen on the client side, the RabbitMQ Stream plugin stores only bytes, it has no idea that AMQP 1.0 message format is used.

AMQP 1.0 message format was chosen because of its flexibility and its advanced type system.
It provides good interoperability, which allows streams to be accessed as AMQP 0-9-1 queues, without data loss.
====

[[outbound-message-deduplication]]
===== Message Deduplication

RabbitMQ Stream provides publisher confirms to avoid losing messages: once the broker has persisted a message it sends a confirmation for this message.
But this can lead to duplicate messages: imagine the connection closes because of a network glitch after the message has been persisted but _before_
the confirmation reaches the producer.
Once reconnected, the producer will retry to send the same message, as it never received the confirmation.
So the message will be persisted twice.

Luckily RabbitMQ Stream can detect and filter out duplicated messages.

The client provides a specific class to handle deduplication: `DeduplicationProducer`.

[[deduplication-multithreading]]
[WARNING]
.Deduplication is not guaranteed when publishing on several threads
====
We'll see below that deduplication works using a strictly increasing sequence for messages.
This means messages must be published in order and the preferred way to do this is usually _within a single thread_.
Even if messages are _created_ in order, with the proper sequence ID, if they are published in several threads, they can get out of order, e.g. message 5 can be _published_ before message 2.
The deduplication mechanism will then filter out message 2 in this case.

So you have to be very careful about the way your applications publish messages when deduplication is in use.
If you worry about performance, note it is possible to publish hundreds of thousands of messages in a single thread with RabbitMQ Stream.
====

====== Use DeduplicationProducer

The `DeduplicationProducer` requires the `Reference` as mandatory parameter.
This parameter enables deduplication:

.Naming a producer to enable message deduplication
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=deduplication-producer]
--------

<1> Define the `DeduplicatingProducer` class with `Reference` property.
<2> Get a message
<3> Send three messages specifying the `publishingid` and the `Message`.
<4> Send again the same message with the same `publishingid` and the `Message` will be skipped by the broker since the `publishingid` is already present with the `Reference` "my_producer_reference" .

Thanks to the name, the broker will be able to track the messages it has persisted on a given stream for this producer.

Consider the `Reference` a logical name.
It should not be a random sequence that changes when the producer application is restarted.
Names like `online-shop-order` or
`online-shop-invoice` are better names than `3d235e79-047a-46a6-8c80-9d159d3e1b05`.
There should be only one living instance of a producer with a given name on a given stream at the same time.

====== Understanding Publishing ID

The `Reference` is only one part of the deduplication mechanism, the other part is the _message publishing ID_.
The publishing ID is a strictly increasing sequence, starting at 0 and incremented for each message.

* the sequence should start at 0
* the sequence must be strictly increasing
* there can be gaps in the sequence (e.g. 0, 1, 2, 3, 6, 7, 9, 10, etc)

A custom publishing ID sequence has usually a meaning: it can be the line number of a file or the primary key in a database.

Note the publishing ID is not part of the message: it is not stored with the message and so is not available when consuming the message.
It is still possible to store the value in the AMQP 1.0 message application properties or in an appropriate properties (e.g. `messageId`).

====== Restarting a Producer Where It Left Off

Using a custom publishing sequence is even more useful to restart a producer where it left off.
Imagine a scenario whereby the producer is sending a message for each line in a file and the application uses the line number as the publishing ID.
If the application restarts because of some necessary maintenance or even a crash, the producer can restart from the beginning of the file: there would no duplicate messages because the producer has a name and the application sets publishing IDs appropriately.
Nevertheless, this is far from ideal, it would be much better to restart just after the last line the broker successfully confirmed.
Fortunately this is possible thanks to the `DeduplicatingProducer#GetLastPublishedId()` method, which returns the last publishing ID for a given producer.
As the publishing ID in this case is the line number, the application can easily scroll to the next line and restart publishing from there.

The next snippet illustrates the use of `DeduplicatingProducer#GetLastPublishedId()`:

.Setting a producer where it left off
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=deduplication-queries-last-publishing-id]
--------

<1> Get a `DeduplicatingProducer` instance
<2> Query last publishing ID for this producer
<3> Use the lastid and increment it

[[sub-entry-batching-and-compression]]
===== Sub-Entry Batching and Compression

RabbitMQ Stream provides a special mode to publish, store, and dispatch messages: sub-entry batching.
This mode increases throughput at the cost of increased latency and potential duplicated messages even when deduplication is enabled.
It also allows using compression to reduce bandwidth and storage if messages are reasonably similar, at the cost of increasing CPU usage on the client side.

Sub-entry batching consists in squeezing several messages – a batch – in the slot that is usually used for one message.
This means outbound messages are not only batched in publishing frames, but in sub-entries as well.

The following snippet shows how to enable sub-entry batching:

.Enabling sub-entry batching
[source,c#,indent=0]
--------
include::{test-examples}/ProducerUsage.cs[tag=producer-sub-entry-batching]
--------

<1> Define a list of messages to compress
<2> Send the list of messages to the broker in tis case 3 messages compressed with GZIP

Reasonable values for the sub-entry size usually go from 10 to a few dozens.

A sub-entry batch will go directly to disc after it reached the broker, so the publishing client has complete control over it.
This is the occasion to take advantage of the similarity of messages and compress them.

The following table lists the supported algorithms, general information about them, and the respective implementations used by default.

[%header,cols=3*]
|===
|Algorithm
|Overview
|Implementation used


|CompressionType.None
|No compression.
|None


|https://en.wikipedia.org/wiki/Gzip[CompressionType.Gzip]
|Has a high compression ratio but is slow compared to other algorithms.
|.Net Implementation

|https://en.wikipedia.org/wiki/Snappy_(compression)[Snappy]
|Aims for reasonable compression ratio and very high speeds.
| Not shipped with the client library, but can be added with the `ICompressionCodec` interface.

|https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)[LZ4]
|Aims for good trade-off between speed and compression ratio.
|Not shipped with the client library, but can be added with the `ICompressionCodec` interface.

|https://en.wikipedia.org/wiki/Zstd[zstd] (Zstandard)
|Aims for high compression ratio and high speed, especially for decompression.
|Not shipped with the client library, but can be added with the `ICompressionCodec` interface.

|===

You are encouraged to test and evaluate the compression algorithms depending on your needs.

[NOTE]
.Consumers, sub-entry batching, and compression
====
There is no configuration required for consumers with regard to sub-entry batching and compression.
The broker dispatches messages to client libraries: they are supposed to figure out the format of messages, extract them from their sub-entry, and decompress them if necessary.
So when you set up sub-entry batching and compression in your publishers, the consuming applications must use client libraries that support this mode, which is the case for the stream Net client.
====

You can add a compression algorithm to the client library by implementing the `ICompressionCodec` interface and registering it with the `StreamCompressionCodecs` class.

The following snippet shows how to add a compression algorithm to the client library:

.Adding a compression algorithm
[source,c#,indent=0]
--------
include::{test-examples}/AddCustomCodec.cs[tag=lz4-i-compression-codec]
--------

<1> Implement the `ICompressionCodec` interface with all the required methods

The following snippet shows how to register the compression algorithm with the `StreamCompressionCodecs` class:

.Registering a compression algorithm
[source,c#,indent=0]
--------
include::{test-examples}/AddCustomCodec.cs[tag=lz4-register-codec]
--------

<1> Register the compression algorithm with the `StreamCompressionCodecs` class
<2> Use the compression algorithm in the `producer.Send(list, CompressionType.Lz4)`

[[creating-a-consumer]]
==== Consumer

`Consumer` is the API to consume messages from a stream.

===== Creating a Consumer

A `Consumer` instance is created with `Consumer.Create(..)`.
The main settings are the stream to consume from, the place in the stream to start consuming from (the _offset_), and a callback when a message is received (the `MessageHandler`).
The next snippet shows how to create a `Consumer`:

.Creating a consumer
[source,c#,indent=0]
--------
include::{test-examples}/ConsumerUsage.cs[tag=consumer-creation]
--------

<1> Use `Consumer.Create()` to define the consumer
<2> Specify `ConsumerConfig` to configure the consumer behavior with the `streamSystem` and `streamName` to consume from
<3> Specify where to start consuming from
<4> Handle the messages
<5> Close consumer after usage

The broker start sending messages as soon as the `Consumer` instance is created.


Staring from the 1.3.0 version, the `Consumer#MessageHandler` API runs in a separated `Task` and it is possible to use `async`/`await` in the handler.


The following table sums up the main settings to create a `Consumer` with `ConsumerConfig`:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Default

|`StreamSystem`
|The StreamSystem to use.
|No default, mandatory setting.

|`Stream`
|The stream to consume from.
|No default, mandatory setting.

|`OffsetSpec`
|The offset to start consuming from.
|`OffsetTypeNext()`

|`MessageHandler`
|The callback for inbound messages.
|No default.

|`Reference`
|The consumer name (for <<consumer-offset-tracking,offset tracking>>.)
|`null` (no offset tracking)



|`ReconnectStrategy`
|The strategy to use when the connection to the broker is lost.
|`BackOffReconnectStrategy`

|`ClientProvidedName`
| To identify the client in the management UI
|`dotnet-stream-conusmer`


|`IsSingleActiveConsumer`
|Enable the Single Active Consumer feature
|`false`

|`IsSuperStream`
|Enable the Super Stream feature
|`false`

|===

[NOTE]
.Why is my consumer not consuming?
====
A consumer starts consuming at the very end of a stream by default (`next` offset).
This means the consumer will receive messages as soon as a producer publishes to the stream.
_This also means that if no producers are currently publishing to the stream, the consumer will stay idle, waiting for new messages to come in_.
See the <<specifying-an-offset,offset section>> to find out more about the different types of offset specification.
====

[[specifying-an-offset]]
===== Specifying an Offset

The offset is the place in the stream where the consumer starts consuming from.
The possible values for the offset parameter are the following:

* `OffsetTypeFirst()`: starting from the first available offset.
If the stream has not been <<limiting-the-size-of-a-stream,truncated>>, this means the beginning of the stream (offset 0).
* `OffsetTypeLast()`: starting from the end of the stream and returning the last <<chunk-definition,chunk>> of messages immediately (if the stream is not empty).
* `OffsetTypeNext()`: starting from the next offset to be written.
Contrary to `OffsetTypeLat()`, consuming with `OffsetTypeNext()`
will not return anything if no-one is publishing to the stream.
The broker will start sending messages to the consumer when messages are published to the stream.
* `OffsetTypeOffset(offset)`: starting from the specified offset. 0 means consuming from the beginning of the stream (first messages).
The client can also specify any number, for example the offset where it left off in a previous incarnation of the application.
* `OffsetTypeTimestamp(timestamp)`: starting from the messages stored after the specified timestamp.
Note consumers can receive messages published a bit before the specified timestamp.
Application code can filter out those messages if necessary.

[[chunk-definition]]
[NOTE]
.What is a chunk of messages?
====
A chunk is simply a batch of messages.
This is the storage and transportation unit used in RabbitMQ Stream, that is messages are stored contiguously in a chunk and they are delivered as part of a chunk.
A chunk can be made of one to several thousands of messages, depending on the ingress.
====

The following figure shows the different offset specifications in a stream made of 2 chunks:

.Offset specifications in a stream made of 2 chunks
[ditaa]
....
   +------------------------------------------+ +-------------------------+
   |  +-----+ +-----+ +-----+ +-----+ +-----+ | | +-----+ +-----+ +-----+ |
   |  |  0  | |  1  | |  2  | |  3  | |  4  | | | |  5  | |  6  | |  7  | |
   |  +-----+ +-----+ +-----+ +-----+ +-----+ | | +-----+ +-----+ +-----+ |
   +------------------------------------------+ +-------------------------+
         ^            Chunk 1    ^                   ^    Chunk 2            ^
         |                       |                   |                       |
       FIRST                  OFFSET 3              LAST                    NEXT
....

Each chunk contains a timestamp of its creation time.
This is this timestamp the broker uses to find the appropriate chunk to start from when using a timestamp specification.
The broker chooses the closest chunk _before_ the specified timestamp, that is why consumers may see messages published a bit before what they specified.

[[consumer-offset-tracking]]
===== Tracking the Offset for a Consumer

RabbitMQ Stream provides server-side offset tracking.
This means a consumer can track the offset it has reached in a stream.
It allows a new incarnation of the consumer to restart consuming where it left off.
All of this without an extra datastore, as the broker stores the offset tracking information.

Offset tracking works in 2 steps:

* the consumer must have a *reference*.
The name is set with `ConsumerConfig#Reference`.
The name can be any value (under 256 characters) and is expected to be unique (from the application point of view).
Note neither the client library, nor the broker enforces uniqueness of the name: if 2 `Consumer` .NET instances share the same name, their offset tracking will likely be interleaved, which applications usually do not expect.
* the consumer must periodically *store the offset* it has reached so far.

Whatever tracking strategy you use, *a consumer must have a `Reference` to be able to store offsets*.

[[consumer-manual-offset-tracking]]
====== Manual Offset Tracking

The manual tracking strategy lets the developer in charge of storing offsets whenever they want, not only after a given number of messages has been received and supposedly processed, like automatic tracking does.

The following snippet shows how to enable manual tracking and how to store the offset at some point:

.Using manual tracking with defaults
[source,c#,indent=0]
--------
include::{test-examples}/ConsumerUsage.cs[tag=manual-tracking-defaults]
--------

<1> Set the consumer Reference (mandatory for offset tracking)
<2> Store the current offset on some condition

The snippet above uses `consumer.StoreOffset(context.Offset)` to store at the offset of the current message.

====== Considerations On Offset Tracking

_When to store offsets?_ Avoid storing offsets too often or, worse, for each message.
Even though offset tracking is a small and fast operation, it will make the stream grow unnecessarily, as the broker persists offset tracking entries in the stream itself.

A good rule of thumb is to store the offset every few thousands of messages.
Of course, when the consumer will restart consuming in a new incarnation, the last tracked offset may be a little behind the very last message the previous incarnation actually processed, so the consumer may see some messages that have been already processed.

A solution to this problem is to make sure processing is idempotent or filter out the last duplicated messages.

'''

_Is the offset a reliable absolute value?_ Message offsets may not be contiguous.
This implies that the message at offset 500 in a stream may not be the 501 message in the stream (offsets start at 0).
There can be different types of entries in a stream storage, a message is just one of them.
For example, storing an offset creates an offset tracking entry, which has its own offset.

This means one must be careful when basing some decision on offset values, like a modulo to perform an operation every X messages.
As the message offsets have no guarantee to be contiguous, the operation may not happen exactly every X messages.

[[single-active-consumer]]
===== Single Active Consumer

WARNING: Single Active Consumer requires *RabbitMQ 3.11* or more.

When the single active consumer feature is enabled for several consumer instances sharing the same stream and name, only one of these instances will be active at a time and so will receive messages.
The other instances will be idle.

The single active consumer feature provides 2 benefits:

* Messages are processed in order: there is only one consumer at a time.
* Consumption continuity is maintained: a consumer from the group will take over if the active one stops or crashes.

A typical sequence of events would be the following:

* Several instances of the same consuming application start up.
* Each application instance registers a single active consumer.
The consumer instances share the same name.
* The broker makes the first registered consumer the active one.
* The active consumer receives and processes messages, the other consumer instances remain idle.
* The active consumer stops or crashes.
* The broker chooses the consumer next in line to become the new active one.
* The new active consumer starts receiving messages.

The next figures illustrates this mechanism.
There can be only one active consumer:

.The first registered consumer is active, the next ones are inactive
[ditaa]
....
                    +----------+
             +------+ consumer + Active
             |      +----------+
             |
+--------+   |      +=---------+
+ stream +---+------+ consumer + Inactive
+--------+   |      +----------+
             |
             |      +=---------+
             +------+ consumer + Inactive
                    +----------+
....

The broker rolls over to another consumer when the active one stops or crashes:

.When the active consumer stops, the next in line becomes active
[ditaa]
....
                    +=---------+
                    | consumer + Closed
                    +----------+

+--------+          +----------+
+ stream +---+------+ consumer + Active
+--------+   |      +----------+
             |
             |      +=---------+
             +------+ consumer + Inactive
                    +----------+
....

Note there can be several groups of single active consumers on the same stream.
What makes them different from each other is the name used by the consumers.
The broker deals with them independently.
Let's use an example.
Imagine 2 different `app-1` and `app-2` applications consuming from the same stream, with 3 identical instances each.
Each instance registers 1 single active consumer with the name of the application.
We end up with 3 `app-1` consumers and 3 `app-2` consumers, 1 active consumer in each group, so overall 6 consumers and 2 active ones, all of this on the same stream.

Let's see now the API for single active consumer.

====== Enabling Single Active Consumer

Use the `ConsumerBuilder#singleActiveConsumer()` method to enable the feature:

.Enabling single active consumer
[source,c#,indent=0]
--------
include::{test-examples}/ConsumerUsage.cs[tag=enabling-single-active-consumer]
--------
<1> Set the `Reference` name (mandatory to enable single active consumer)
<2> Enable single active consumer

With the configuration above, the consumer will take part in the `application-1` group on the `my-stream` stream.
If the consumer instance is the first in a group, it will get messages as soon as there are some available.
If it is not the first in the group, it will remain idle until it is its turn to be active (likely when all the instances registered before it are gone).

====== Offset Tracking

Single active consumer and offset tracking work together: when the active consumer goes away, another consumer takes over and you need to tell the client library where to resume from and you can do this by implementing the `ConsumerUpdateListener` API.

[[consumer-update-listener]]
====== Reacting to Consumer State Change

The broker notifies a consumer that becomes active before dispatching messages to it.
The broker expects a response from the consumer and this response contains the offset the dispatching should start from.
So this is the consumer's responsibility to compute the appropriate offset, not the broker's.
The default behavior is to look up the last stored offset for the consumer on the stream.
This works when server-side offset tracking is in use, but it does not when the application chose to use an external store for offset tracking.
In this case, it is possible to use the `ConsumerConfig#ConsumerUpdateListener()` method like demonstrated in the following snippet:

.Fetching the last stored offset from an external store in the consumer update listener callback
[source,c#,indent=0]
--------
include::{test-examples}/ConsumerUsage.cs[tag=sac-consumer-update-listener]
--------

<1> Set the `Reference` name (mandatory to enable single active consumer)
<2> Enable single active consumer
<3> Handle `ConsumerUpdateListener` callback


[[low-high-level-classes]]
==== Low Level and High Level classes

.NET stream client provides  two types of classes:

* Low-level classes
* High-level classes

===== Low-level classes

** `RabbitMQ.Stream.Client.RawProducer` - Low-level producer class
** `RabbitMQ.Stream.Client.RawConsumer`  - Low-level consumer class
** `RabbitMQ.Stream.Client.RawSuperStreamProducer` - Low-level super-stream producer class
** `RabbitMQ.Stream.Client.RawSuperStreamConsumer` - Low-level super-stream consumer class

The Classes are used to interact with the stream server in a low level way. They are used to create streams, publish messages, consume messages, etc. They give you all the callbacks to manually handle events like:

* `Disconnection`
* `Metadata update`

.Creating a Raw Producer
[source,c#,indent=0]
--------
include::{test-examples}/RawClasses.cs[tag=raw-producer-creation]
--------
<1> Create a `RawProducer` instance
<2> Event in case of disconnection
<3> Event in case of MetadataHandler update. This event is triggered by the server when a stream changes topology like deleted or added/removed mirrors
<4> ConfirmHandler event. This event is triggered when a `PublishingId` message is confirmed by the server with or without an error.

Like the `RawProducer` class, the `Raw*` classes have the same events to handle the disconnection and metadata update.

It is up to the user to handle the disconnection and metadata update events.

[WARNING]
.Be careful when using the `Raw*` classes.
====
They are low-level classes and you need to handle the disconnection and metadata update events. If you don't handle them, you will end up with a disconnected client and you will not be able to reconnect to the server.

"RawProducer:send" is not thread-safe. You need to synchronize access to it.
"RawProducer" does not handle the timeout/error confirmation messages. You need to handle it yourself.
====

===== High-level classes

** <<creating-a-producer, `Producer`>> - High-level producer class 
** <<creating-a-consumer, `Consumer`>> - High-level consumer class


`Producer` and `Consumer` classes handle auto-reconnection, metadata updates, super-stream and some low-level client behaviour.

The `Producer` traces the sent and received messages to give back to the user the original message sent to the server and also handle the message timeout. See <<confirmation-status>> for more details.

It would be best to use `Producer` and `Consumer` classes unless you need to handle the low-level details.  

